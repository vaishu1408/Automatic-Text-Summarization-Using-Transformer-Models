# -*- coding: utf-8 -*-
"""Text Project

Automatically generated by Colab.

Visit the link below to see as Jupyter Notebook

Original file is located at
    https://colab.research.google.com/drive/1obustYXSDp18Rs_a4QV2KgXDkxGkMRl-
"""

#The following code has been worked upon by all the members in a collaborative way (idea or code or fixing errors), hence not tagging code as part of individual contribution

# Installing necessary libraries as needed

#!pip install transformers rouge-score optuna datasets

# Importing libraries after installing
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, get_scheduler
from rouge_score import rouge_scorer
import optuna
from datasets import load_dataset

# Checking if GPU is available
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

# Load and preprocess the dataset
# We are dropping blank empty rows in data
# We are converting the data to string to not have any issues later

def load_data(filepath):
    data = pd.read_csv(filepath)
    data['text'] = data['text'].apply(str)
    data['headlines'] = data['headlines'].apply(str)
    return data

# Dataset Class with On-the-Fly Tokenization
class NewsDataset(Dataset):
    def __init__(self, data, tokenizer, max_input_length=512, max_output_length=128):

        """
        Initializes the NewsDataset object.

        Parameters:
        - data: The dataset, typically a Pandas DataFrame containing the text articles and their summaries (headlines).
        - tokenizer: The tokenizer object (e.g., from Hugging Face) used to preprocess text into token IDs.
        - max_input_length: Maximum token length for input articles
        - max_output_length: Maximum token length for output headlines

        """

        self.data = data
        self.tokenizer = tokenizer
        self.max_input_length = max_input_length
        self.max_output_length = max_output_length

    def __len__(self):

        # Returns the number of data samples in the dataset.
        return len(self.data)

    def __getitem__(self, idx):

        """
        Retrieves a single data sample at the specified index, processes it through the tokenizer, and returns tokenized input and output with attention masks.

        Parameters: 1. idx: The index of the sample to retrieve.

        Returns:
        A dictionary with the following keys:
        1. 'input_ids': Token IDs of the input article, padded/truncated to max_input_length.
        2. 'attention_mask': Attention mask for the input article, indicating non-padded tokens.
        3. 'labels': Token IDs of the output headline, padded/truncated to max_output_length.

        """

        # Extract the article text and corresponding headline from the dataset
        article = self.data.iloc[idx]['text']
        headline = self.data.iloc[idx]['headlines']

        # Tokenize the article for input, with padding and truncation applied
        input_tokens = self.tokenizer(
            article, max_length=self.max_input_length, padding="max_length", truncation=True, return_tensors="pt"
        )

        # Tokenize the headline for output, with padding and truncation applied
        output_tokens = self.tokenizer(
            headline, max_length=self.max_output_length, padding="max_length", truncation=True, return_tensors="pt"
        )

        # Return the tokenized data in a dictionary format
        return {
            'input_ids': input_tokens['input_ids'].squeeze(),
            'attention_mask': input_tokens['attention_mask'].squeeze(),
            'labels': output_tokens['input_ids'].squeeze()
        }


# Evaluation Function
def evaluate_model(model, data_loader, tokenizer):

    """
    Evaluates a model on a given dataset using ROUGE scores.

    Parameters:
    1. model: The trained model to be evaluated.
    2. data_loader: DataLoader object providing batches of tokenized input/output data.
    3. tokenizer: Tokenizer used for decoding model outputs and labels.

    Returns:
    avg_scores: A dictionary containing average ROUGE scores ('rouge1', 'rouge2', 'rougeL').

    """

    # Set the model to evaluation mode (disables dropout and similar layers)
    model.eval()

    # Initialize a ROUGE scorer for evaluating generated text
    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
    total_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}
    count = 0

    # Disable gradient computation for evaluation

    with torch.no_grad():

        # Iterate through batches in the DataLoader
        for batch in data_loader:

            # Move input data and labels to the appropriate device
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            # Generate predictions from the model
            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128)

            # Decode predictions and references (labels) into text
            predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            references = tokenizer.batch_decode(labels, skip_special_tokens=True)

            # Calculate ROUGE scores for each prediction-reference pair
            for pred, ref in zip(predictions, references):
                scores = rouge.score(pred, ref)
                for key in total_scores.keys():
                    total_scores[key] += scores[key].fmeasure
                count += 1

    # Calculate average ROUGE scores across all samples
    avg_scores = {k: v / count for k, v in total_scores.items()}
    print(f"ROUGE Scores: {avg_scores}")
    return avg_scores

# Optuna for Hyperparameter Tuning
def objective(trial):
    # Initialize tokenizer inside the function
    tokenizer = T5Tokenizer.from_pretrained("t5-base")

    # Hyperparameter space
    learning_rate = trial.suggest_float("learning_rate", 1e-5, 1e-4, log=True)
    batch_size = trial.suggest_categorical("batch_size", [2, 4])
    max_epochs = 1  # Quick testing for tuning

    # Data Preparation
    train_data = load_data('news_summary_more.csv').sample(n=5000, random_state=42)
    train_dataset = NewsDataset(train_data, tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Model, Optimizer, Scheduler
    model = T5ForConditionalGeneration.from_pretrained("t5-base").to(device)
    optimizer = AdamW(model.parameters(), lr=learning_rate)
    scheduler = get_scheduler("linear", optimizer=optimizer, num_warmup_steps=100, num_training_steps=len(train_loader) * max_epochs)

    # Training Loop
    model.train()
    for epoch in range(max_epochs):
        for batch in train_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()


    # Evaluation
    test_data = load_data('news_summary_more.csv').sample(n=1000, random_state=42)
    test_dataset = NewsDataset(test_data, tokenizer)
    test_loader = DataLoader(test_dataset, batch_size=2)
    scores = evaluate_model(model, test_loader, tokenizer)
    return scores['rouge1']  # Optimize for ROUGE-1


# Run Hyperparameter Optimization
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=5)

# Best Hyperparameters
print("Best Hyperparameters:", study.best_params)
tokenizer = T5Tokenizer.from_pretrained("t5-base")

# Fine-tuning with Best Hyperparameters
best_params = study.best_params
train_data = load_data('news_summary_more.csv')
train_dataset = NewsDataset(train_data, tokenizer)
train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True)

model = T5ForConditionalGeneration.from_pretrained("t5-base").to(device)
optimizer = AdamW(model.parameters(), lr=best_params['learning_rate'])

for epoch in range(3):  # Full training
    print(f"Epoch {epoch + 1}/3")
    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

# Save the model
model.save_pretrained("saved_model")
tokenizer.save_pretrained("saved_model")

# Final Evaluation
test_data = load_data('news_summary_more.csv').sample(n=1000, random_state=42)
test_dataset = NewsDataset(test_data, tokenizer)
test_loader = DataLoader(test_dataset, batch_size=2)
final_scores = evaluate_model(model, test_loader, tokenizer)
print("Final ROUGE Scores:", final_scores)

# Generating summaries for the first 10 test data points
model.eval()
print("\nFirst 10 Generated Summaries:")
for idx, row in test_data.head(10).iterrows():
    input_text = row['text']
    input_ids = tokenizer.encode(input_text, return_tensors="pt", max_length=256, truncation=True).to(device)
    summary_ids = model.generate(input_ids, max_length=64, min_length=10, length_penalty=2.0, num_beams=4)
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    print(f"\nTest Case {idx + 1}:")
    print(f"Original Text: {row['text']}")
    print(f"Generated Summary: {summary}")
    print(f"Reference Summary: {row['headlines']}")